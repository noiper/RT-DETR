import torch
import torch.nn as nn
import torch.nn.functional as F
import copy
from typing import Dict, List, Tuple, Optional
from ..rtdetr.rtdetrv2_decoder import RTDETRTransformerv2, TransformerDecoder

class TemporalFusionBlock(nn.Module):
    """
    Fusion block for combining non-key frame features (S) with cached key frame features (CCFF)
    """
    def __init__(self, s_channels: int, hidden_dim: int):
        super().__init__()
        self.s_channels = s_channels
        self.hidden_dim = hidden_dim
        
        # Project S features to hidden_dim
        self.s_proj = nn.Sequential(
            nn.Conv2d(s_channels, hidden_dim, kernel_size=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True)
        )
        
        # Fusion layer (combines projected S with CCFF)
        self.fusion = nn.Sequential(
            nn.Conv2d(hidden_dim * 2, hidden_dim, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=1, bias=False),
            nn.BatchNorm2d(hidden_dim)
        )
        
    def forward(self, s_feat: torch.Tensor, ccff_feat: torch.Tensor) -> torch.Tensor:
        """
        Fuse S features with cached CCFF features
        
        Args:
            s_feat: Non-key frame feature [B, s_channels, H, W]
            ccff_feat: Cached key frame feature [B, hidden_dim, H, W]
        
        Returns:
            fused_feat: Fused feature [B, hidden_dim, H, W]
        """
        # Project S to hidden_dim
        s_proj = self.s_proj(s_feat)  # [B, hidden_dim, H, W]
        
        # Concatenate and fuse
        concat = torch.cat([s_proj, ccff_feat], dim=1)  # [B, hidden_dim*2, H, W]
        fused = self.fusion(concat)  # [B, hidden_dim, H, W]
        
        # Residual connection
        fused = fused + ccff_feat
        
        return fused

class LightweightDecoder(RTDETRTransformerv2):
    """
    - REQUIRES query_emb and pos_emb from key frame
    - No denoising
    - No aux_loss
    """
    def __init__(self, full_decoder: RTDETRTransformerv2, num_layers: int = 1):
        nn.Module.__init__(self)
        
        # self.hidden_dim = full_decoder.hidden_dim       
        self.num_levels = full_decoder.num_levels
        # self.num_classes = full_decoder.num_classes
        # self.num_queries = full_decoder.num_queries

        # Copy decoder layers
        self.num_decoder_layers = min(num_layers, full_decoder.decoder.num_layers)
        self.decoder = TransformerDecoder(full_decoder.hidden_dim, full_decoder.decoder.layers[0], self.num_decoder_layers)
        
        self.input_proj = copy.deepcopy(full_decoder.input_proj)
        self.query_pos_head = copy.deepcopy(full_decoder.query_pos_head)
        self.dec_score_head = nn.ModuleList([
            copy.deepcopy(full_decoder.dec_score_head[i]) for i in range(num_layers)
        ])
        self.dec_bbox_head = nn.ModuleList([
            copy.deepcopy(full_decoder.dec_bbox_head[i]) for i in range(num_layers)
        ])
        
        self.eval_spatial_size = full_decoder.eval_spatial_size
    
    def _get_encoder_input(self, feats: List[torch.Tensor]):
        # get projection features
        proj_feats = [self.input_proj[i](feat) for i, feat in enumerate(feats)]
        if self.num_levels > len(proj_feats):
            len_srcs = len(proj_feats)
            for i in range(len_srcs, self.num_levels):
                if i == len_srcs:
                    proj_feats.append(self.input_proj[i](feats[-1]))
                else:
                    proj_feats.append(self.input_proj[i](proj_feats[-1]))

        # get encoder inputs
        feat_flatten = []
        spatial_shapes = []
        for i, feat in enumerate(proj_feats):
            _, _, h, w = feat.shape
            # [b, c, h, w] -> [b, h*w, c]
            feat_flatten.append(feat.flatten(2).permute(0, 2, 1))
            # [num_levels, 2]
            spatial_shapes.append([h, w])
        # [b, l, c]
        feat_flatten = torch.concat(feat_flatten, 1)
        return feat_flatten, spatial_shapes

    def forward(self, feats, cached_content, cached_points_unact):
        """
        Forward pass using cached query embeddings from key frame
        
        Args:
            feats: List of multi-scale features [feat1, feat2, feat3]
            cached_content: Cached content from key frame [B, hidden_dim, H, W]
            cached_points_unact: Cached reference points from key frame [B, num_queries, 4] (REQUIRED)
        
        Returns:
            outputs: Dict with 'pred_logits' and 'pred_boxes' only
        """
        # Get input proj
        memory, spatial_shapes = self._get_encoder_input(feats)
        
        out_bboxes, out_logits = self.decoder(
            cached_content,
            cached_points_unact,
            memory,
            spatial_shapes,
            self.dec_bbox_head,
            self.dec_score_head,
            self.query_pos_head,
            attn_mask=None,
        )
        out = {'pred_logits': out_logits[-1], 'pred_boxes': out_bboxes[-1]}

        # pred_logits = output['pred_logits']
        # pred_boxes = output['pred_boxes']
        
        return out

class TemporalRTDETR(nn.Module):
    """
    Temporal RT-DETR for Phase 1 training
    - Key frame: Backbone + Encoder + Decoder
    - Non-key frame: Backbone + Fusion + Lightweight Decoder
    """
    def __init__(
        self,
        backbone: nn.Module,
        encoder: nn.Module,
        decoder: nn.Module,
        num_classes: int = 80,
        hidden_dim: int = 256,
        num_queries: int = 300,
        use_lightweight_decoder: bool = True,
        reuse_queries: bool = True,
    ):
        super().__init__()
        
        self.backbone = backbone
        self.encoder = encoder
        self.decoder = decoder
        self.num_classes = num_classes
        self.hidden_dim = hidden_dim
        self.num_queries = num_queries
        self.use_lightweight_decoder = use_lightweight_decoder
        self.reuse_queries = reuse_queries
        
        # Cached features from key frame
        self.cached_ccff = None
        self.cached_content = None
        self.cached_points_unact = None

        device = next(decoder.parameters()).device

        self.fusion_blocks = nn.ModuleList([
            TemporalFusionBlock(s_channels=128, hidden_dim=hidden_dim).to(device),  # S3 + CCFF1
            TemporalFusionBlock(s_channels=256, hidden_dim=hidden_dim).to(device),  # S4 + CCFF2
            TemporalFusionBlock(s_channels=512, hidden_dim=hidden_dim).to(device),  # S5 + CCFF3
        ])

        # Create lightweight decoder if needed
        if use_lightweight_decoder:
            self.lightweight_decoder = LightweightDecoder(
                full_decoder=decoder,
                num_layers=1
            )
        else:
            self.lightweight_decoder = None
        
        print(f"  Success!")
        print(f"  - Use lightweight decoder: {use_lightweight_decoder}")
        print(f"  - Reuse queries: {reuse_queries}")
    
    def forward_key_frame(self, img: torch.Tensor, targets: Optional[List[Dict]] = None) -> Tuple:
        """
        Forward key frame through full pipeline and cache features
        
        Args:
            img: Key frame image [B, C, H, W]
            targets: Ground truth annotations
        
        Returns:
            outputs: Detection outputs
            ccff_features: Cached multi-scale features
            query_embeddings: Cached query embeddings (optional)
        """
        backbone_features = self.backbone(img)
        c3, c4, c5 = backbone_features[-3:]
        encoder_output = self.encoder([c3, c4, c5])
        self.cached_ccff = [feat.detach() for feat in encoder_output]
        outputs, cached_query = self.decoder(encoder_output, return_query=True, targets=targets)
        
        if self.reuse_queries:
            # self.cached_queries = outputs['query_embed'].detach()
            self.cached_content = cached_query[0][:, :self.num_queries, :].detach()
            self.cached_points_unact = cached_query[1][:, :self.num_queries, :].detach()
        
        return outputs
    
    def forward_non_key_frame(self, img: torch.Tensor, targets: Optional[List[Dict]] = None) -> Dict:
        """
        Forward non-key frame through lightweight pipeline with fusion
        
        Args:
            img: Non-key frame image [B, C, H, W]
            targets: Ground truth annotations
        
        Returns:
            outputs: Detection outputs
        """
        if self.cached_ccff is None:
            raise RuntimeError("Key frame must be processed first to cache CCFF features")
        
        # Extract multi-scale features from backbone
        backbone_features = self.backbone(img)
        
        # Get last 3 scales: S3, S4, S5
        s3, s4, s5 = backbone_features[-3:]
        s_features = [s3, s4, s5]
        
        # Fuse each scale with cached CCFF
        fused_features = []
        for _, (s_feat, ccff_feat, fusion_block) in enumerate(zip(s_features, self.cached_ccff, self.fusion_blocks)):
            fused = fusion_block(s_feat, ccff_feat)
            fused_features.append(fused)
        
        # Prepare decoder input (fused multi-scale features)
        decoder_input = fused_features
        
        # Get cached query embeddings if available
        # query_embed = self.cached_content if self.reuse_queries else None
        
        # Use lightweight or full decoder
        if self.use_lightweight_decoder and self.lightweight_decoder is not None:
            # Use single-layer decoder (trainable)
            # Call with only positional argument (memory)
            outputs = self.lightweight_decoder(decoder_input, self.cached_content, self.cached_points_unact)
        else:
            # Use full decoder
            outputs = self.decoder(decoder_input, targets=targets)
        
        return outputs
    
    def forward(self, key_frame: torch.Tensor, non_key_frame: torch.Tensor, 
                key_targets: Optional[List[Dict]] = None, 
                non_key_targets: Optional[List[Dict]] = None) -> Tuple[Dict, Dict]:
        """
        Forward both key and non-key frames
        
        Args:
            key_frame: Key frame image
            non_key_frame: Non-key frame image
            key_targets: Key frame targets
            non_key_targets: Non-key frame targets
        
        Returns:
            key_outputs, non_key_outputs
        """
        # Process key frame
        key_outputs = self.forward_key_frame(key_frame, key_targets)
        
        # Process non-key frame
        non_key_outputs = self.forward_non_key_frame(non_key_frame, non_key_targets)
        
        return key_outputs, non_key_outputs


def build_temporal_rtdetr(cfg):
    """Build Temporal RT-DETR model from config"""
    # Import backbone, encoder, decoder builders
    from ..rtdetr import build_backbone, build_encoder, build_decoder
    
    # Build components
    backbone = build_backbone(cfg)
    encoder = build_encoder(cfg)
    decoder = build_decoder(cfg)
    
    # Build temporal model
    model = TemporalRTDETR(
        backbone=backbone,
        encoder=encoder,
        decoder=decoder,
        num_classes=cfg.num_classes,
        hidden_dim=cfg.hidden_dim,
        num_queries=cfg.num_queries,
        use_lightweight_decoder=cfg.get('use_lightweight_decoder', True),
        reuse_queries=cfg.get('reuse_queries', True),
    )
    
    return model
